{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf6e286-3994-464e-b913-5c882518f365",
   "metadata": {},
   "source": [
    "## Building a Decision Tree From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ab3fe-7c14-40cd-b7c1-bb7e2027d1ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-Calculate entropy\n",
    "\n",
    "Write a function `compute_entropy` that computes the entropy (measure of impurity) at a node. \n",
    "- The function takes in a numpy array (`y`) that indicates whether the examples in that node are (`1`) or (`0`) \n",
    "\n",
    "Complete the `compute_entropy()` function below to:\n",
    "* Compute $p_1$, which is the fraction of examples that are edible (i.e. have value = `1` in `y`)\n",
    "* The entropy is then calculated as \n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "* Note \n",
    "    * The log is calculated with base $2$\n",
    "    * For implementation purposes, $0\\text{log}_2(0) = 0$. That is, if `p_1 = 0` or `p_1 = 1`, set the entropy to `0`\n",
    "    * Make sure to check that the data at a node is not empty (i.e. `len(y) != 0`). Return `0` if it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0196f95c-c7af-464f-a412-7687d3c8dc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for \n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): Numpy array indicating whether each example at a node is\n",
    "           edible (`1`) or poisonous (`0`)\n",
    "       \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node\n",
    "        \n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = 0\n",
    "    for i in y:\n",
    "        if i == 1:\n",
    "            count += 1\n",
    "    p1 = count / len(y)\n",
    "    \n",
    "    if p1 == 0 or p1 == 1:\n",
    "        entropy = 0\n",
    "    else:\n",
    "        entropy = -1 * p1 * np.log2(p1) - (1-p1) * np.log2(1-p1)\n",
    "        \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef116459-f59b-4836-aa7d-34adb0c52552",
   "metadata": {},
   "source": [
    "### 2-Split dataset\n",
    "\n",
    "Next write a  function called `split_dataset` that takes in the data at a node and a feature to split on and splits it into left and right branches.\n",
    "\n",
    "- The function takes in the training data, the list of indices of data points at that node, along with the feature to split on. \n",
    "- It splits the data and returns the subset of indices at the left and the right branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf480777-6a0b-4f51-b12b-9eb30d5faf2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into\n",
    "    left and right branches\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (list):     Indices with feature value == 1\n",
    "        right_indices (list):    Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    left_indices = [i for i in node_indices if X[i][feature] == 1]\n",
    "    right_indices = [i for i in node_indices if X[i][feature] == 0]    \n",
    "\n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b1658-cf27-4fc5-8cf2-0997aa4441d7",
   "metadata": {},
   "source": [
    "### 3-Calculate information gain\n",
    "\n",
    "Next write a function called `information_gain` that takes in the training data, the indices at a node and a feature to split on and returns the information gain from the split.\n",
    "\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$\n",
    "\n",
    "where \n",
    "- $H(p_1^\\text{node})$ is entropy at the node \n",
    "- $H(p_1^\\text{left})$ and $H(p_1^\\text{right})$ are the entropies at the left and the right branches resulting from the split\n",
    "- $w^{\\text{left}}$ and $w^{\\text{right}}$ are the proportion of examples at the left and right branch, respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1688c7-c27c-4282-8a24-6c0748dd53e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "   \n",
    "    Returns:\n",
    "        cost (float):        Cost computed\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Split dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    \n",
    "    information_gain = compute_entropy(y_node) - ((w_left * compute_entropy(y_left))+\n",
    "                                                  (w_right * compute_entropy(y_right)))\n",
    "   \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240dd47-5a93-4164-b2dc-0a128e82fa99",
   "metadata": {},
   "source": [
    "### 4-Get best split\n",
    " write a function `get_best_split()` to get the best feature to split on by computing the information gain from each feature as we did above and returning the feature that gives the maximum information gain\n",
    "\n",
    "- The function takes in the training data, along with the indices of datapoint at that node\n",
    "- The output of the function is the feature that gives the maximum information gain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e60b19-8ab2-4243-b588-facf4f69d2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "    \n",
    "    \n",
    "\n",
    "    best_gain = 0\n",
    "    for i in range(num_features):\n",
    "        gain = compute_information_gain(X, y, node_indices, i)\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = i\n",
    "    \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80954e0-8e53-4b61-a215-98ea82c32bbc",
   "metadata": {},
   "source": [
    "### 5 - Building the tree\n",
    "\n",
    "we use the functions you implemented above to generate a decision tree by successively picking the best feature to split on until we reach the stopping criteria (maximum depth is 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e106d9f-4543-4578-be73-8d645c8a7349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = []\n",
    "\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree. \n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb782e-117c-4e93-9c8f-325ce370b60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8918d-927e-4abd-8f47-eed96eb3b695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
